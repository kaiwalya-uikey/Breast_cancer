{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Classification / Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing lib\n",
    "import numpy as np\n",
    "# NumPy (short for Numerical Python) is a popular Python library for numerical and scientific computing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# panda particularly for data preprocessing and data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearClassification(object):\n",
    "    # The LinearClassification class has two main methods: fit() and predict(). \n",
    "    # The fit() method is used to train the linear classification model on a given dataset, \n",
    "    # while the predict() method is used to predict the class labels of new data points.\n",
    "\n",
    "    def __init__(self):\n",
    "        # In the __init__ method, we initialize the weights and bias to None. \n",
    "        # In the fit() method, we compute the weights and bias using gradient descent, \n",
    "        # and in the predict() method, we use the learned weights and bias to make predictions.\n",
    "\n",
    "        # Overall, the __init__ method is a key component of object-oriented programming in Python, \n",
    "        # and is used to initialize the state of objects when they are created.\n",
    "\n",
    "        #defining hyperparams\n",
    "        self.learning_rate = 0.0001\n",
    "        self.batch_size = 200\n",
    "        self.no_of_iter = 1000\n",
    "        #videcu za ovaj\n",
    "        self.reg = 0.000001\n",
    "        \n",
    "    \n",
    "    #Input NOTE: X - matrix of data, can be used on images or numerical data (N x D)\n",
    "    #          N - Number of samples, D - Number of features\n",
    "    #          In case you use images make sure that X.shape[0] represent NUMBER of samples\n",
    "    #          y - labels (Nx1)\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        #0 notation - so we add + 1 to max value from y\n",
    "        self.no_of_classes = np.max(y) + 1\n",
    "        # In the context of a machine learning classification problem, the code \n",
    "        # self.no_of_classes = np.max(y) + 1 sets the number of classes in the problem.\n",
    "\n",
    "    # y is the target variable or label for the classification problem and it contains \n",
    "    # the true class labels for each observation in the dataset. The np.max() function \n",
    "    # returns the maximum value in y, which is the highest class label in the dataset. \n",
    "    # Since class labels start from 0, we add 1 to get the total number of classes in the problem.\n",
    "\n",
    "    # By setting self.no_of_classes to this value, we are creating an attribute for the \n",
    "    # LinearClassification class that stores the number of classes in the problem. \n",
    "    # This attribute can then be used in other methods of the class, such as the predict() method,\n",
    "    #  to ensure that the predicted class labels are within the range of possible class labels.\n",
    "\n",
    "    # For example, if self.no_of_classes is set to 3, then the predicted class labels should be 0, 1, or 2. \n",
    "    # If a predicted label is outside this range, it is not a valid class label for the problem and should be treated as an error.\n",
    "        \n",
    "        #defining hyperparams\n",
    "        # W - matrix of weights (No_of_classes x No_of_features)\n",
    "        self.W = np.random.rand(self.no_of_classes, self.X_train.shape[1]) * 0.001\n",
    "        # initializes the weight matrix W with random values.\n",
    "        #  The np.random.rand() function returns an array of random values between 0 and 1 with the specified shape.\n",
    "        # By setting self.W to this random weight matrix, we are creating an attribute for the LinearClassification \n",
    "        # class that stores the initial weights for \n",
    "        # the classification model. This attribute will be updated during the training process to\n",
    "        #  improve the accuracy of the model on the training data\n",
    "        \n",
    "        self.W, loss_history = self.SGD(self.W, self.X_train, self.y_train, self.learning_rate, self.batch_size, self.no_of_iter, self.reg)\n",
    "        # In the context of a machine learning classification problem, the code \n",
    "        # self.W, loss_history = self.SGD(self.W, self.X_train, self.y_train, self.learning_rate, self.batch_size, self.no_of_iter, self.reg)\n",
    "        #  starts the stochastic gradient descent (SGD) algorithm to train the classification model.\n",
    "\n",
    "# self.W is the weight matrix for the classification model that was initialized earlier. self.X_train and self.y_train \n",
    "# are the training data and corresponding target labels, respectively. self.learning_rate, self.batch_size, self.no_of_iter, \n",
    "# and self.reg are hyperparameters that control the behavior of the SGD algorithm.\n",
    "\n",
    "# The self.SGD() method is a function defined within the LinearClassification class that implements the SGD algorithm.\n",
    "#  This method takes the current weight matrix W, training data X_train and y_train, learning rate learning_rate,\n",
    "#  batch size batch_size, number of iterations no_of_iter, and regularization strength reg as inputs, and returns \n",
    "# the updated weight matrix and a list of loss values over the training iterations.\n",
    "\n",
    "# The self.W attribute is updated with the new weight matrix returned by the self.SGD() method, and the\n",
    "#  loss_history variable is assigned to the list of loss values returned by the self.SGD() method.\n",
    "\n",
    "# By calling the self.SGD() method and updating the weight matrix and loss history, we are training the \n",
    "# LinearClassification model on the training data using the SGD algorithm.\n",
    "        return loss_history\n",
    "    \n",
    "    #STOCHASTIC GRADIENT DESCENT\n",
    "    #Inputs: W - weights that we are trying to update\n",
    "    #        X - feautere of training set\n",
    "    #        y - wanted labels\n",
    "    #        learning_rate - how fast it is going to find good parameters\n",
    "    #        batch_size - how big PART of training set algo is using per iter\n",
    "    #        no_of_iter -  how many times it is going to run\n",
    "    #        reg - regularization\n",
    "    #\n",
    "    #Outputs: W_updated - updated weights matrix acording to loss function used\n",
    "    #         loss_history for verbose reptresentation of our loss computation\n",
    "    def SGD(self, W, X, y, learning_rate, batch_size, no_of_iter, reg):\n",
    "        W_updated = W\n",
    "        \n",
    "        no_of_train = X.shape[0]\n",
    "        #It is not necessities, but we can define loss_hitory to be sure that algo is working good\n",
    "        loss_history = []\n",
    "        \n",
    "        for i in range(no_of_iter):\n",
    "            batch_inx = np.random.choice(no_of_train, batch_size, replace=True)\n",
    "            #creting smallers train sets to fit in our SGD\n",
    "            X_batch = X[batch_inx,:]\n",
    "            y_batch = y[batch_inx]\n",
    "            \n",
    "            \n",
    "            loss, grad = self.SVM_classfier(W_updated, X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "            #Update W:\n",
    "            W_updated = W_updated - (learning_rate * grad)\n",
    "            \n",
    "        return W_updated, loss_history\n",
    "            \n",
    "    #Inputs: W - current weights\n",
    "    #        X - training set features\n",
    "    #        y - training set labels\n",
    "    #        reg - regularization strenght\n",
    "    #\n",
    "    #Outputs: gradient_W - values to updated starting W\n",
    "    #         loss - to see if we are updaing in good direction\n",
    "    def SVM_classfier(self, W, X, y, reg):\n",
    "        \n",
    "        no_of_classes = np.max(y) + 1\n",
    "        #creating matrix with zeros, same shape as starting weights\n",
    "        \n",
    "        gradient_W = np.zeros(W.shape)\n",
    "        \n",
    "        loss = 0.0 \n",
    "        for i in range(X.shape[0]):\n",
    "            #First we need to multiply weights and x for particular sample\n",
    "            #need to transpose to long vector current sample\n",
    "            scores = W.dot(X[i, :].T)\n",
    "            #we are getting values for currect class\n",
    "            correct_class = scores[y[i]]\n",
    "            for j in range(no_of_classes):\n",
    "                if j == y[i]:\n",
    "                    continue\n",
    "                # This is simple formula for SVM\n",
    "                current_class_margin = scores[j] - correct_class + 1 #one is \n",
    "                if current_class_margin > 0:\n",
    "                    loss +=  current_class_margin\n",
    "                \n",
    "                    gradient_W[y[i]:1, :] -= X[i, :] #This is where we are creating gradient for CURRECT class\n",
    "                    gradient_W[j:1, :] += X[y[i], :]\n",
    "        \n",
    "        #average over number of train samples\n",
    "        loss /= X.shape[0]\n",
    "        gradient_W /= X.shape[0]\n",
    "        \n",
    "        loss += 0.5 * reg * np.sum(W * W)\n",
    "        \n",
    "        gradient_W += reg*W\n",
    "    \n",
    "        return loss, gradient_W\n",
    "    \n",
    "    #Predict function\n",
    "    #Input: X - test set \n",
    "    #\n",
    "    #Output: predict - list of classes\n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for i in range(X.shape[0]):\n",
    "            pred.append(np.argmax(np.dot(self.W,X[i, :].T)))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to check how much did algo predict right\n",
    "def accuracy(y_tes, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_tes[i] == y_pred[i]):\n",
    "            correct += 1\n",
    "    return (correct/len(y_tes))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Importing the dataset\n",
    "    dataset = pd.read_csv('breastCancer.csv')\n",
    "    dataset.replace('?', 0, inplace=True)\n",
    "    dataset = dataset.applymap(np.int64)\n",
    "    X = dataset.iloc[:, 1:-1].values    \n",
    "    y = dataset.iloc[:, -1].values\n",
    "    #handling labels column\n",
    "    y_new = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 2:\n",
    "            y_new.append(0)\n",
    "        else:\n",
    "            y_new.append(1)\n",
    "    y_new = np.array(y_new)\n",
    "\n",
    "    \n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "\n",
    "    # Feature Scaling\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     sc = StandardScaler()\n",
    "#     X_train = sc.fit_transform(X_train)\n",
    "#     X_test = sc.transform(X_test)\n",
    "\n",
    "   \n",
    "    classifier = LinearClassification()\n",
    "    loss_history = classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    #Sklearn test\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    reg = LogisticRegression(random_state=0)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_sk = reg.predict(X_test)\n",
    "\n",
    "# Uncomment if you want to print out losses\n",
    "#     for i in range(len(loss_history)):\n",
    "#         print(loss_history[i])\n",
    "    \n",
    "    print(\"My algorithm on this dataset: \",accuracy(y_test, y_pred), \"%\")\n",
    "    print(\"Sklearn Logistic regression score: \",accuracy(y_test, y_pred_sk),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My algorithm on this dataset:  67.42857142857143 %\n",
      "Sklearn Logistic regression score:  96.57142857142857 %\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
